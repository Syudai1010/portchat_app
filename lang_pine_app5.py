import streamlit as st
import os
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone as LangChainPinecone
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.set_page_config(page_title="æ¸¯æ¹¾ã®æ–½è¨­ã®æŠ€è¡“ä¸Šã®åŸºæº– ChatBot", page_icon="ğŸ’¬")
st.title(" ğŸ“˜æ¸¯æ¹¾ã®æ–½è¨­ã®æŠ€è¡“ä¸Šã®åŸºæº–ğŸ“– ChatBotğŸ’¬")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸRetrieval-Augmented Generation (RAG) ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
Pinecone ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¨ OpenAI ã® GPT ã‚’çµ„ã¿åˆã‚ã›ã€éå»ã®å±¥æ­´ã‚’è¸ã¾ãˆã¦å¯¾è©±ã‚’è¡Œã„ã¾ã™ã€‚
""")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
st.sidebar.title("ğŸ›  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
temperature = st.sidebar.slider("Temperature (ç”Ÿæˆã®å¤šæ§˜æ€§)", 0.0, 1.0, 0.7, step=0.1)
top_k = st.sidebar.slider("Top-k Documents (æ¤œç´¢æ™‚ã®ä¸Šä½æ–‡æ›¸æ•°)", 1, 10, 3)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
openai_api_key = os.getenv("OPENAI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_ENVIRONMENT")
index_name = os.getenv("PINECONE_INDEX_NAME")

# Pinecone ã®åˆæœŸåŒ–
if pinecone_api_key:
    try:
        pc = Pinecone(api_key=pinecone_api_key)
        if index_name not in [index.name for index in pc.list_indexes()]:
            pc.create_index(
                name=index_name,
                dimension=1536,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region=pinecone_env)
            )
            st.success(f"æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"Pinecone åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.error("Pinecone API Key ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state["history"] = []  # åˆæœŸåŒ–

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹
st.subheader("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
user_input = st.text_input("è³ªå•", placeholder="ä¾‹: æœ€è¿‘ã®AIæŠ€è¡“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ï¼Ÿ")

if user_input and openai_api_key and pinecone_api_key:
    # OpenAI Embeddingsã®åˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # Pinecone ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
    vector_store = LangChainPinecone.from_existing_index(
        index_name=index_name, embedding=embeddings
    )

    # OpenAI LLM ã®åˆæœŸåŒ–
    llm = OpenAI(openai_api_key=openai_api_key, temperature=temperature, max_tokens=1500)

    # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’ã¾ã¨ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    conversation_history = "\n".join(
        [f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {assistant}" for user, assistant in st.session_state["history"]]
    )

    # ç¾åœ¨ã®è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    if conversation_history:
        prompt = f"{conversation_history}\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"
    else:
        prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"

    # RetrievalQA ã®ä½œæˆ
    retriever = vector_store.as_retriever(search_kwargs={"k": top_k})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, retriever=retriever, chain_type="stuff", return_source_documents=True  # ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã‚‚è¿”ã™ã‚ˆã†ã«è¨­å®š
    )

    # è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¨å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã‚’å–å¾—
    result = qa_chain({"query": prompt})

    response = result['result']  # å›ç­”éƒ¨åˆ†
    source_documents = result['source_documents']  # å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è³ªå•ã¨å›ç­”ã‚’è¿½åŠ 
    st.session_state["history"].append((user_input, response))

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    with st.expander("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´", expanded=True):
        for i, (user, assistant) in enumerate(st.session_state["history"]):
            st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼ ({i+1})**: {user}")
            st.markdown(f"**ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ**: {assistant}")

    # å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®è¡¨ç¤º
    st.subheader("ğŸ” å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®ä¸€éƒ¨:")
    for doc in source_documents:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ–‡æ›¸åã¨ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—
        source_name = doc.metadata.get("source", "æ–‡æ›¸åä¸æ˜")
        page_number = doc.metadata.get("page", "ãƒšãƒ¼ã‚¸ç•ªå·ä¸æ˜")
        
        st.markdown(f"**æ–‡æ›¸å**: {source_name}")
        st.markdown(f"**ãƒšãƒ¼ã‚¸ç•ªå·**: {page_number}")
        st.markdown(f"**æ–‡æ›¸å†…å®¹**: {doc.page_content}")  # å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®å†…å®¹ã‚’è¡¨ç¤º
